{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\zhizh\\appdata\\local\\conda\\conda\\envs\\tf-gpu\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = os.listdir('set_vid')\n",
    "count = 0\n",
    "import ffmpeg\n",
    "#for v in vid:\n",
    "    #os.rename('set_vid/' + v, 'set_vid/' + v[0:9] + str(count) + '.h264') \n",
    "    #count += 1\n",
    "    #os.system(\"ffmpeg -i set_vid/\" + v + \" set_vid/\" + v[0:-5] + \".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = cv2.imread(\"set2/2w.jpg\")\n",
    "# im = cv2.resize(im,(28,28))\n",
    "# cv2.imwrite(\"resized-2w.jpg\", im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save images as numpy array\n",
    "directory_list = ['set3','set2','set1']\n",
    "directory_list2 = ['set3','set2','set1']\n",
    "train_array = []\n",
    "train_label = []\n",
    "eval_array = []\n",
    "eval_label = []\n",
    "\n",
    "'''\n",
    "0 - w\n",
    "1 - a\n",
    "2 - s\n",
    "3 - d\n",
    "4 - stop\n",
    "[1,0,0,0]\n",
    "'''\n",
    "\n",
    "def return_label(inp):\n",
    "    if (inp == 'w'):\n",
    "        return np.array([1.0,0,0,0,0])\n",
    "        #return np.array([0])\n",
    "    elif (inp == 'a'):\n",
    "        return np.array([0,1.0,0,0,0])\n",
    "        #return np.array([1])\n",
    "    elif (inp == 's'):\n",
    "        return np.array([0,0,1.0,0,0])\n",
    "        #return np.array([2])\n",
    "    elif (inp == 'd'):\n",
    "        return np.array([0,0,0,1.0,0])\n",
    "        #return np.array([3])\n",
    "    elif (inp == 'x'):\n",
    "        return np.array([0,0,0,0,1.0])\n",
    "        #return np.array([4])\n",
    "\n",
    "for d in directory_list:\n",
    "    img_list = os.listdir(d)\n",
    "    for i in img_list:\n",
    "        im = cv2.imread(d + \"/\" + i)\n",
    "        im = cv2.resize(im,(32,32))\n",
    "        train_array.append(im)\n",
    "        train_label.append(return_label(i[i.find('.')-1]))\n",
    "\n",
    "        \n",
    "        \n",
    "for d in directory_list2:\n",
    "    img_list = os.listdir(d)\n",
    "    for i in img_list:\n",
    "        im = cv2.imread(d + \"/\" + i)\n",
    "        im = cv2.resize(im,(32,32))\n",
    "        eval_array.append(im)\n",
    "        eval_label.append(return_label(i[i.find('.')-1]))\n",
    "directory_list3 = ['/stop','/straight','/turn_left','/turn_right']\n",
    "\n",
    "count = 0\n",
    "for d in directory_list3:\n",
    "    img_list = os.listdir('set_vid'+d)\n",
    "    for i in img_list:\n",
    "        im = cv2.imread('set_vid' + d + '/' + i)\n",
    "        im = cv2.resize(im,(32,32))\n",
    "        train_array.append(im)\n",
    "        if (count == 0):\n",
    "            train_label.append(np.array([0,0,0,0,1.0]))\n",
    "        elif (count == 1):\n",
    "            train_label.append(np.array([1.0,0,0,0,0]))\n",
    "        elif (count == 2):\n",
    "            train_label.append(np.array([0,1.0,0,0,0]))\n",
    "        elif (count == 3):\n",
    "            train_label.append(np.array([0,0,0,1.0,0]))\n",
    "        \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = 5 # Number of different types of labels (1-10)\n",
    "WIDTH = 32 # width / height of the image\n",
    "CHANNELS = 3 # Number of colors in the image (greyscale)\n",
    "\n",
    "VALID = 30 # Validation data size\n",
    "\n",
    "STEPS = 20000 #20000   # Number of steps to run\n",
    "BATCH = 100 # Stochastic Gradient Descent batch size\n",
    "PATCH = 5 # Convolutional Kernel size\n",
    "DEPTH = 12 #32 # Convolutional Kernel depth size == Number of Convolutional Kernels\n",
    "HIDDEN = 100 #1024 # Number of hidden neurons in the fully connected layer\n",
    "\n",
    "LR = 0.001 # Learning rate\n",
    "\n",
    "tf_data = tf.placeholder(tf.float32, shape=(None, WIDTH, WIDTH, CHANNELS))\n",
    "tf_labels = tf.placeholder(tf.float32, shape=(None, LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.truncated_normal([PATCH, PATCH, CHANNELS, DEPTH], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([DEPTH]))\n",
    "w2 = tf.Variable(tf.truncated_normal([PATCH, PATCH, DEPTH, 2*DEPTH], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(1.0, shape=[2*DEPTH]))\n",
    "w3 = tf.Variable(tf.truncated_normal([WIDTH // 4 * WIDTH // 4 * 2*DEPTH, HIDDEN], stddev=0.1))\n",
    "b3 = tf.Variable(tf.constant(1.0, shape=[HIDDEN]))\n",
    "w4 = tf.Variable(tf.truncated_normal([HIDDEN, LABELS], stddev=0.1))\n",
    "b4 = tf.Variable(tf.constant(1.0, shape=[LABELS]))\n",
    "\n",
    "def logits(data):\n",
    "    # Convolutional layer 1\n",
    "    x = tf.nn.conv2d(data, w1, [1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    x = tf.nn.relu(x + b1)\n",
    "    # Convolutional layer 2\n",
    "    x = tf.nn.conv2d(x, w2, [1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    x = tf.nn.relu(x + b2)\n",
    "    # Fully connected layer\n",
    "    x = tf.reshape(x, (-1, WIDTH // 4 * WIDTH // 4 * 2*DEPTH))\n",
    "    x = tf.nn.relu(tf.matmul(x, w3) + b3)\n",
    "    return tf.matmul(x, w4) + b4\n",
    "\n",
    "# Prediction:\n",
    "tf_pred = tf.nn.softmax(logits(tf_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits(tf_data), \n",
    "                                                                 labels=tf_labels))\n",
    "tf_acc = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(tf_pred, 1), tf.argmax(tf_labels, 1))))\n",
    "\n",
    "#tf_opt = tf.train.GradientDescentOptimizer(LR)\n",
    "#tf_opt = tf.train.AdamOptimizer(LR)\n",
    "tf_opt = tf.train.RMSPropOptimizer(LR)\n",
    "tf_step = tf_opt.minimize(tf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "train_data = np.asarray(train_array, dtype=np.float32)\n",
    "train_labels = np.asarray(train_label, dtype=np.int32)\n",
    "\n",
    "valid_data = np.asarray(eval_array, dtype=np.float32)\n",
    "valid_labels = np.asarray(eval_label, dtype=np.int32)\n",
    "\n",
    "\n",
    "#indices = np.random.permutation(data.shape[0])\n",
    "#train_data, valid_data = data[:int(len(indices)*.8)],data[int(len(indices)*.8):]\n",
    "\n",
    "#indices2 = np.random.permutation(labels.shape[0])\n",
    "#train_labels, valid_labels = labels[:int(len(indices)*.8)],labels[int(len(indices)*.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 \t Valid. Acc. = 61.395348\n",
      "Step 1000 \t Valid. Acc. = 84.031006\n",
      "Step 1500 \t Valid. Acc. = 91.782944\n",
      "Step 2000 \t Valid. Acc. = 97.984497\n",
      "Step 2500 \t Valid. Acc. = 99.069771\n",
      "Step 3000 \t Valid. Acc. = 97.829453\n",
      "Step 3500 \t Valid. Acc. = 91.937981\n",
      "Step 4000 \t Valid. Acc. = 97.519379\n",
      "Step 4500 \t Valid. Acc. = 93.798447\n",
      "Step 5000 \t Valid. Acc. = 97.054260\n",
      "Step 5500 \t Valid. Acc. = 97.519379\n",
      "Step 6000 \t Valid. Acc. = 98.139534\n",
      "Step 6500 \t Valid. Acc. = 97.674416\n",
      "Step 7000 \t Valid. Acc. = 98.759689\n",
      "Step 7500 \t Valid. Acc. = 94.883720\n",
      "Step 8000 \t Valid. Acc. = 99.069771\n",
      "Step 8500 \t Valid. Acc. = 99.689926\n",
      "Step 9000 \t Valid. Acc. = 98.139534\n",
      "Step 9500 \t Valid. Acc. = 99.534882\n",
      "Step 10000 \t Valid. Acc. = 99.689926\n",
      "Step 10500 \t Valid. Acc. = 99.844963\n",
      "Step 11000 \t Valid. Acc. = 98.604652\n",
      "Step 11500 \t Valid. Acc. = 99.534882\n",
      "Step 12000 \t Valid. Acc. = 99.379845\n",
      "Step 12500 \t Valid. Acc. = 96.744186\n",
      "Step 13000 \t Valid. Acc. = 99.224808\n",
      "Step 13500 \t Valid. Acc. = 99.379845\n",
      "Step 14000 \t Valid. Acc. = 97.364342\n",
      "Step 14500 \t Valid. Acc. = 98.449615\n",
      "Step 15000 \t Valid. Acc. = 99.379845\n",
      "Step 15500 \t Valid. Acc. = 99.534882\n",
      "Step 16000 \t Valid. Acc. = 99.534882\n",
      "Step 16500 \t Valid. Acc. = 98.759689\n",
      "Step 17000 \t Valid. Acc. = 99.689926\n",
      "Step 17500 \t Valid. Acc. = 99.689926\n",
      "Step 18000 \t Valid. Acc. = 98.759689\n",
      "Step 18500 \t Valid. Acc. = 99.534882\n",
      "Step 19000 \t Valid. Acc. = 99.689926\n",
      "Step 19500 \t Valid. Acc. = 99.534882\n",
      "Step 20000 \t Valid. Acc. = 99.379845\n"
     ]
    }
   ],
   "source": [
    "ss = ShuffleSplit(n_splits=STEPS, train_size=BATCH)\n",
    "ss.get_n_splits(train_data, train_labels)\n",
    "history = [(0, np.nan, 10)] # Initial Error Measures\n",
    "for step, (idx, _) in enumerate(ss.split(train_data,train_labels), start=1):\n",
    "    fd = {tf_data:train_data[idx], tf_labels:train_labels[idx]}\n",
    "    session.run(tf_step, feed_dict=fd)\n",
    "    if step%500 == 0:\n",
    "        fd = {tf_data:valid_data, tf_labels:valid_labels}\n",
    "        valid_loss, valid_accuracy = session.run([tf_loss, tf_acc], feed_dict=fd)\n",
    "        history.append((step, valid_loss, valid_accuracy))\n",
    "        print('Step %i \\t Valid. Acc. = %f'%(step, valid_accuracy), end='\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projectx\\\\makeathon2019\\\\model_2'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, os.path.abspath(os.path.join(os.getcwd(), 'model_2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
